# ViTDet v11 (loss-focused) sweep
#
# Goal:
# - Optimize `val_loss_5d_weighted` (mode=min) in a *consistent* target space.
# - Keep the v5 two-stage structure (stage1 single-split + ASHA, stage2 k-fold eval),
#   but tighten the search space using empirical signals from:
#     outputs/tune_analysis/experiments_summary.csv
#     outputs/tune_analysis/param_importance.md
#
# Run:
#   python tune.py --config-name tune_vitdet_v11_loss

defaults:
  - tune_vitdet_v5
  - _self_

version: tune-vitdet-v11-loss

# IMPORTANT: keep target space consistent when using loss as the Tune objective.
# (Tuning log_scale_targets would mix objective scales and bias selection.)
model:
  log_scale_targets: false

tune:
  name: tune-vitdet-v11-loss

  metric: val_loss_5d_weighted
  mode: min

  # Keep v5's budget by default; override from CLI if desired.
  num_samples: 120

  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    grace_period: 12
    reduction_factor: 2

  seeds: [42]
  report_per_epoch: true

  two_stage:
    enabled: true
    scope: best
    min_epoch: ${tune.scheduler.grace_period}
    top_n: 6
    name_suffix: "-stage2-kfold"

    # Keep the same objective in stage 2 (k-fold).
    stage2_metric: val_loss_5d_weighted
    stage2_mode: min

    train_overrides:
      kfold.enabled: true
      kfold.k: 5
      kfold.even_split: false
      kfold.folds: null
      kfold.group_by_date_state: true
      train_all.enabled: false
      data.val_split: 0.0
      trainer.max_epochs: ${trainer.max_epochs}

  # Search space deltas vs v5 (override specific keys to keep config small).
  search_space:
    # ---- Scheduler ----
    scheduler.warmup_epochs:
      type: choice
      values: [0, 1, 3]

    # ---- Backbone multi-layer fusion ----
    model.backbone_layers.indices_by_backbone.dinov3_vith16plus:
      type: choice
      values:
        - [24, 31]
        - [28, 29, 30, 31]
        - [12, 24, 31]

    # ---- ViTDet head ----
    model.head.vitdet_dim:
      type: choice
      values: [320, 512]

    model.head.hidden_dims:
      type: choice
      values:
        - []
        - [1280]

    # Keep dropout broad (it showed high importance), but preserve v5's bounds.
    model.head.dropout:
      type: uniform
      lower: 0.15
      upper: 0.55

    # ---- LoRA ----
    peft.lora_alpha:
      type: choice
      values: [8, 16]

    # Narrow lora_lr upper range (lower values looked more stable on average).
    peft.lora_lr:
      type: loguniform
      lower: 2.0e-04
      upper: 1.5e-03

    # Avoid near-zero dropout (higher adapter dropout tended to be more stable).
    peft.lora_dropout:
      type: uniform
      lower: 0.03
      upper: 0.15

    # Expose LoRA LLRD as a small discrete knob (keeps combinations bounded).
    peft.lora_llrd:
      type: choice
      values: [0.8, 0.8729858381132479, 0.9, 0.95, 1.0]

    # ---- Manifold MixUp ----
    data.augment.manifold_mixup.detach_pair:
      type: choice
      values: [false, true]

