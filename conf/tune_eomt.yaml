defaults:
  # Base training config (same as other Tune runs)
  - experiment@_global_: baseline_no_irish
  # Switch to EoMT-style head for this sweep
  - override /model/head: eomt
  # Enable LoRA for HPO runs (the baseline experiment keeps it disabled by default).
  - override /train/peft: lora
  - _self_

hydra:
  job:
    chdir: false
  output_subdir: null
  run:
    dir: .

# HPO should run a single split by default (fast). After selecting best config,
# rerun `train.py --config <best_cfg.yaml>` (or `train_hydra.py`) with train_all/kfold enabled.
kfold:
  enabled: false
  folds: null
train_all:
  enabled: false

trainer:
  max_epochs: 40
  limit_train_batches: 1500
  log_every_n_steps: 10
  checkpoint:
    every_n_epochs: 1
  # Prefer EMA during HPO so per-epoch `val_r2` reflects the averaged model (less noisy, better aligned
  # with final exported weights). Keep SWA disabled to avoid mixing averaging strategies.
  swa:
    enabled: false
  ema:
    enabled: true
    decay: 0.999
    update_every_n_steps: 1
    start_step: 0
    eval_with_ema: true
    apply_at_end: true
    trainable_only: true

data:
  # Reduce loader contention when running multiple trials across the 2 machines.
  num_workers: 8
  prefetch_factor: 2
  # Train:val = 8:2
  val_split: 0.2

ray:
  # For a Ray cluster started via `ray start ...`, use `auto`.
  # For Ray Client, use: "ray://<head_ip>:10001"
  address: auto

# Fixed (non-search) defaults.
model:
  backbone_layers:
    layer_fusion: learned
  head:
    eomt:
      # Keep the legacy (cheap) representation by default: use mean pooled query output.
      # Additional sources (mean_patch/cls) can be enabled via Tune search_space below.
      use_mean_query: true

peft:
  use_dora: false
  target_modules: [qkv]
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  lora_llrd: 0.9

tune:
  name: tune-eomt-v1
  resume: false
  restore_path: null
  resume_unfinished: true
  resume_errored: true
  restart_errored: false
  storage_path: ray_results

  # Optimize r^2 directly (Ray metric name matches Lightning log name).
  metric: val_loss_5d_weighted
  mode: max

  # Smaller search space => fewer samples needed; override from CLI if you want longer.
  num_samples: 40
  max_concurrent_trials: 2
  resources_per_trial:
    cpu: 8
    gpu: 1

  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    # Keep a decent minimum training length before pruning (epoch reporting is 1-based).
    grace_period: 15
    reduction_factor: 2

  seeds: [42]
  report_per_epoch: true

  # Two-stage filtering (best cost/perf):
  # - Stage 1 (cheap): single split HPO (this file sets kfold.enabled=false).
  # - Stage 2 (expensive): take the top-N configs from stage 1 and rerun them with k-fold enabled.
  two_stage:
    enabled: true

    # How to rank stage-1 trials when selecting top-N:
    # - scope=best: pick the best metric value seen during training (after min_epoch)
    # - scope=last: pick the last metric value seen (after min_epoch)
    scope: best
    # Ignore very early noisy epochs; default to ASHA grace_period.
    min_epoch: ${tune.scheduler.grace_period}
    # Number of configs promoted to stage 2.
    top_n: 8

    # Stage-2 Tune run naming (stored under the same tune.storage_path).
    name_suffix: "-stage2-kfold"

    # Optional: optimize a different metric in stage 2 (defaults to tune.metric/mode).
    # stage2_metric: val_r2_global
    # stage2_mode: max

    # Stage-2 resume knobs (OFF by default because the candidate set is derived from stage 1).
    resume: false
    restore_path: null
    resume_unfinished: true
    resume_errored: true
    restart_errored: false

    # Stage-2 resources (defaults to stage-1 settings).
    max_concurrent_trials: ${tune.max_concurrent_trials}
    resources_per_trial: ${tune.resources_per_trial}

    # Fixed training overrides applied during stage 2 (on top of base config + sampled params).
    train_overrides:
      kfold.enabled: true
      kfold.k: 5
      kfold.even_split: false
      kfold.folds: null
      train_all.enabled: false

  # Reduced search space (keep only the high-impact dimensions).
  # Keys are dotted paths into the training config.
  search_space:
    # ---- Optimizer / scheduler ----
    optimizer.lr:
      type: loguniform
      lower: 1.0e-5
      upper: 3.0e-3
    optimizer.weight_decay:
      type: loguniform
      lower: 1.0e-6
      upper: 3.0e-3
    scheduler.warmup_epochs:
      type: choice
      values: [1, 3, 5]

    # ---- EoMT head ----
    # EoMT uses an injected-query head + a scalar MLP on pooled features.
    model.head.hidden_dims:
      type: choice
      values:
        - [512, 256]
        - [1024, 512]
        - [512]
        - [1024]
    model.head.activation:
      type: choice
      values: [relu, silu]
    model.head.dropout:
      type: uniform
      lower: 0.05
      upper: 0.35

    model.head.eomt.num_queries:
      type: choice
      values: [1, 8, 16, 32, 64]
    model.head.eomt.num_blocks:
      type: choice
      values: [1, 2, 4, 6, 8]
    model.head.eomt.query_pool:
      type: choice
      values: [mean, first]

    # Enable extra pooled sources on top of mean_query (kept enabled via fixed defaults above).
    model.head.eomt.use_mean_patch:
      type: choice
      values: [false, true]
    model.head.eomt.use_cls_token:
      type: choice
      values: [false, true]

    # Projection for concatenated sources before the scalar MLP.
    model.head.eomt.proj_dim:
      type: choice
      values: [0, 256, 512, 640, 1280]
    model.head.eomt.proj_activation:
      type: choice
      values: [relu, silu]
    model.head.eomt.proj_dropout:
      type: uniform
      lower: 0.0
      upper: 0.2

    # ---- LoRA ----
    peft.last_k_blocks:
      type: choice
      values: [2, 4, 6, 12, 16]
    peft.r:
      type: choice
      values: [2, 4, 8, 16]
    peft.lora_alpha:
      type: choice
      values: [4, 8, 16]
    peft.lora_lr:
      type: loguniform
      lower: 3.0e-5
      upper: 3.0e-3
    peft.lora_dropout:
      type: uniform
      lower: 0.0
      upper: 0.1
    peft.lora_llrd:
      type: choice
      values: [1.0, 0.9, 0.8]

