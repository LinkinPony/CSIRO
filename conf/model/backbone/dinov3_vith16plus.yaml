# @package model

backbone: dinov3_vith16plus
weights_url: null
# Provide a local path to an offline checkpoint to avoid any network access.
weights_path: dinov3_weights/dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pt
pretrained: true
freeze_backbone: true

# Cast *frozen* backbone weights to lower precision to save VRAM.
backbone_weights_dtype: fp32

# Enable activation/gradient checkpointing on DINOv3 transformer blocks to reduce VRAM
# at the cost of a slower backward pass. Useful when training LoRA with limited GPU memory.
gradient_checkpointing: false

# IMPORTANT: must match backbone's embed_dim:
# - dinov3_vitl16     -> 1024
# - dinov3_vith16plus -> 1280
# - dinov3_vit7b16    -> 4096
embedding_dim: 1280

log_scale_targets: false


