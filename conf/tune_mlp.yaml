defaults:
  # Base training config (same as other Tune runs)
  - experiment@_global_: baseline_no_irish
  # (Explicit) MLP head sweep
  - override /model/head: mlp
  # Enable LoRA for HPO runs (the baseline experiment keeps it disabled by default).
  - override /train/peft: lora
  - _self_

hydra:
  job:
    chdir: false
  output_subdir: null
  run:
    dir: .

# HPO should run a single split by default (fast). After selecting best config,
# rerun `train.py --config <best_cfg.yaml>` (or `train_hydra.py`) with train_all/kfold enabled.
kfold:
  enabled: false
  folds: null
train_all:
  enabled: false

trainer:
  max_epochs: 45
  limit_train_batches: 1500
  log_every_n_steps: 10
  checkpoint:
    every_n_epochs: 1
  # Prefer EMA during HPO so per-epoch `val_r2` reflects the averaged model (less noisy, better aligned
  # with final exported weights). Keep SWA disabled to avoid mixing averaging strategies.
  swa:
    enabled: false
  ema:
    enabled: true
    decay: 0.999
    update_every_n_steps: 1
    start_step: 0
    eval_with_ema: true
    apply_at_end: true
    trainable_only: true

data:
  # Reduce loader contention when running multiple trials across the 2 machines.
  num_workers: 8
  prefetch_factor: 2
  # Train:val = 8:2
  val_split: 0.2

ray:
  # For a Ray cluster started via `ray start ...`, use `auto`.
  # For Ray Client, use: "ray://<head_ip>:10001"
  address: auto

# Fixed (non-search) defaults based on the r^2 analysis of previous sweeps:
# - Avoid GELU (high failure rate at larger LRs)
# - Prefer qkv-only LoRA modules (best mean r^2)
# - Disable DoRA (small negative average effect)
model:
  backbone_layers:
    layer_fusion: learned

peft:
  use_dora: false
  target_modules: [qkv]
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  lora_llrd: 0.9

tune:
  name: tune-mlp-v6
  resume: false
  restore_path: null
  resume_unfinished: true
  resume_errored: true
  restart_errored: false
  storage_path: ray_results

  # Optimize r^2 directly (Ray metric name matches Lightning log name).
  metric: val_r2
  mode: max

  # Smaller search space => fewer samples needed; override from CLI if you want longer.
  num_samples: 40
  max_concurrent_trials: 2
  resources_per_trial:
    cpu: 8
    gpu: 1
  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    # Keep a decent minimum training length before pruning (epoch reporting is 1-based).
    grace_period: 15
    reduction_factor: 2

  seeds: [42]
  report_per_epoch: true

  # Two-stage filtering (best cost/perf):
  # - Stage 1 (cheap): single split HPO (this file already sets kfold.enabled=false).
  # - Stage 2 (expensive): take the top-N configs from stage 1 and rerun them with k-fold enabled
  #   (more reliable selection, better aligned with Kaggle/LB correlation).
  two_stage:
    enabled: true

    # How to rank stage-1 trials when selecting top-N:
    # - scope=best: pick the best metric value seen during training (after min_epoch)
    # - scope=last: pick the last metric value seen (after min_epoch)
    scope: best
    # Ignore very early noisy epochs; default to ASHA grace_period.
    min_epoch: ${tune.scheduler.grace_period}
    # Number of configs promoted to stage 2.
    top_n: 8

    # Stage-2 Tune run naming (stored under the same tune.storage_path).
    name_suffix: "-stage2-kfold"

    # Optional: optimize a different metric in stage 2 (defaults to tune.metric/mode).
    # This can improve correlation with Kaggle when using a more LB-aligned metric (e.g. val_r2_global).
    # stage2_metric: val_r2_global
    # stage2_mode: max

    # Stage-2 resume knobs (OFF by default because the candidate set is derived from stage 1).
    resume: false
    restore_path: null
    resume_unfinished: true
    resume_errored: true
    restart_errored: false

    # Stage-2 resources (defaults to stage-1 settings).
    max_concurrent_trials: ${tune.max_concurrent_trials}
    resources_per_trial: ${tune.resources_per_trial}

    # Fixed training overrides applied during stage 2 (on top of base config + sampled params).
    # You can restrict folds by setting `kfold.folds: "0,1,2"` (or an int / list).
    train_overrides:
      kfold.enabled: true
      kfold.k: 5
      kfold.even_split: false
      kfold.folds: null
      train_all.enabled: false

  # Reduced search space (keep only the high-impact dimensions).
  # Keys are dotted paths into the training config.
  search_space:
    # ---- Optimizer / scheduler ----
    optimizer.lr:
      type: loguniform
      lower: 1.0e-5
      upper: 3.0e-3   # prune unstable high-LR region from previous sweeps
    optimizer.weight_decay:
      type: loguniform
      lower: 1.0e-6
      upper: 3.0e-3
    scheduler.warmup_epochs:
      type: choice
      values: [1, 3, 5]

    # ---- MLP head ----
    model.head.hidden_dims:
      type: choice
      values:
        - []          # linear head on pooled features
        - [320]
        - [640]
        - [1280]
        - [512, 256]
        - [1024, 512]
    model.head.activation:
      type: choice
      values: [relu, silu]
    model.head.dropout:
      type: uniform
      lower: 0.05
      upper: 0.35
    model.head.ratio_head_mode:
      type: choice
      values: [shared, separate_mlp, separate_spatial]

    # ---- LoRA ----
    peft.last_k_blocks:
      type: choice
      values: [4, 6, 12, 16]
    # LoRA capacity (rank) + scaling (alpha). These two together control adapter expressivity
    # and effective update scale (alpha/r). Keep the range small for fast ASHA pruning.
    peft.r:
      type: choice
      values: [4, 8, 16]
    peft.lora_alpha:
      type: choice
      values: [4, 8, 16]
    peft.lora_lr:
      type: loguniform
      lower: 3.0e-5
      upper: 3.0e-3
    # Regularization / stability knobs (often high impact for regression generalization).
    peft.lora_dropout:
      type: uniform
      lower: 0.0
      upper: 0.1
    # LoRA layer-wise LR decay (1.0 disables). Helps when adapting many blocks.
    peft.lora_llrd:
      type: choice
      values: [1.0, 0.9, 0.8]


