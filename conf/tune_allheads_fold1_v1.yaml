# Hardest-fold (fold1) multi-head sweep:
# - Stage 1: k-fold enabled but run ONLY fold_1 (seed=42), with ASHA + per-epoch reporting.
# - Stage 2: promote top-N configs and re-evaluate with full 5-fold, select by val_r2_global.
#
# Run:
#   python tune.py --config-name tune_allheads_fold1_v1
#
# Tip (cluster/NFS):
#   python tune.py --config-name tune_allheads_fold1_v1 tune.storage_path=/mnt/csiro_nfs/ray_results ray.address=auto

defaults:
  # Reuse v10 training defaults (augmentation/LoRA/backbone fusion), but override kfold + search space here.
  - tune_vitdet_v10
  - _self_

version: tune-allheads-fold1-v1

# -----------------------------
# Stage-1 fold selection (hardest fold)
# -----------------------------
kfold:
  enabled: true
  k: 5
  even_split: false
  # IMPORTANT: only run fold_1 during Stage-1 HPO.
  folds: [1]
  group_by_date_state: true

data:
  # Not used when k-fold is enabled (splits are explicit), but keep explicit to avoid confusion.
  val_split: 0.0

trainer:
  # Keep the short budget you used recently; stage-2 reruns full k-fold at the same budget.
  max_epochs: 25

tune:
  name: tune-allheads-fold1-v1

  # Stage-1 ASHA objective (per-epoch):
  # - Use val_r2 because our Ray callback guarantees it is always present.
  metric: val_r2
  mode: max

  # Default budget; override from CLI if you want a longer sweep.
  num_samples: 120

  # Keep Stage-1 single-seed so ASHA semantics stay clean.
  seeds: [42]
  report_per_epoch: true

  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    grace_period: 10
    reduction_factor: 2

  two_stage:
    enabled: true
    # Prefer configs that are good at the end of the fixed budget (helps reduce peak->last mismatch).
    scope: last
    # Require trials to reach late epochs so early-pruned runs don't dominate promotion.
    min_epoch: 20
    top_n: 8
    name_suffix: "-stage2-kfold"

    # Stage-2 selection metric (full k-fold): align to the competition-style global baseline.
    stage2_metric: val_r2_global
    stage2_mode: max

    train_overrides:
      # Full k-fold in stage 2
      kfold.enabled: true
      kfold.k: 5
      kfold.even_split: false
      kfold.folds: null
      kfold.group_by_date_state: true
      # Keep train_all disabled (k-fold already covers the whole dataset across folds).
      train_all.enabled: false
      # Not used under k-fold, kept explicit.
      data.val_split: 0.0
      # Keep epoch budget consistent across stages by default.
      trainer.max_epochs: ${trainer.max_epochs}

  # Search space: multi-head but biased to historically strong / stable regions.
  # Keys are dotted paths into the training config.
  search_space:
    _delete_: true

    # ---- Head type (multi-head) ----
    model.head.type:
      type: choice
      values: [vitdet, mamba, mlp, eomt]

    # ---- Target space ----
    model.log_scale_targets:
      type: choice
      values: [false, true]

    # ---- Backbone multi-layer fusion ----
    model.backbone_layers.layer_fusion:
      type: choice
      values: [mean, learned]
    model.backbone_layers.indices_by_backbone.dinov3_vith16plus:
      type: choice
      values:
        - [28, 29, 30, 31]
        - [24, 31]
        - [31]
        - [12, 24, 31]

    # ---- Optimizer / scheduler ----
    optimizer.lr:
      type: loguniform
      lower: 1.0e-05
      upper: 6.0e-04
    optimizer.weight_decay:
      type: loguniform
      lower: 1.0e-06
      upper: 3.0e-03
    scheduler.warmup_epochs:
      type: choice
      values: [0, 1, 3, 5]

    # ---- Shared head knobs (apply across head types) ----
    model.head.hidden_dims:
      type: choice
      values:
        - []          # linear head
        - [256]
        - [512]
        - [1280]
        - [512, 256]
    model.head.activation:
      type: choice
      values: [silu, relu]
    model.head.dropout:
      type: uniform
      lower: 0.05
      upper: 0.60
    model.head.ratio_head_mode:
      type: choice
      # Keep it cheap for HPO; separate_spatial doubles head compute.
      values: [shared, separate_mlp]

    # ---- ViTDet head ----
    model.head.vitdet_dim:
      type: choice
      values: [256, 320, 512]
    model.head.vitdet_scale_factors:
      type: choice
      values:
        - [2.0, 1.0, 0.5]
        - [2.0, 1.0, 0.5, 0.25]

    # ---- Mamba head (2D mixing) ----
    model.head.mamba.dim:
      type: choice
      values: [192, 256, 320]
    model.head.mamba.depth:
      type: choice
      values: [2, 3]
    model.head.mamba.d_conv:
      type: choice
      values: [3, 5, 7]
    model.head.mamba.bidirectional:
      type: choice
      values: [false, true]

    # ---- EoMT head (keep small to avoid exploding combinations) ----
    model.head.eomt.num_queries:
      type: choice
      values: [16]
    model.head.eomt.num_blocks:
      type: choice
      values: [2, 4]
    model.head.eomt.query_pool:
      type: choice
      values: [mean]
    model.head.eomt.use_mean_patch:
      type: choice
      values: [false, true]
    model.head.eomt.proj_dropout:
      type: uniform
      lower: 0.0
      upper: 0.10

    # ---- LoRA / DoRA ----
    peft.use_dora:
      type: choice
      values: [false, true]
    peft.target_modules:
      type: choice
      values:
        - [qkv]
        - [qkv, proj]
        - [qkv, proj, w1, w2, w3]
    peft.r:
      type: choice
      values: [4, 8, 16]
    peft.lora_alpha:
      type: choice
      values: [4, 8, 16, 32]
    peft.lora_lr:
      type: loguniform
      lower: 3.0e-05
      upper: 3.0e-03
    peft.lora_dropout:
      type: uniform
      lower: 0.0
      upper: 0.15
    peft.last_k_blocks:
      type: choice
      values: [8, 12, 16]
    peft.lora_llrd:
      type: choice
      values: [1.0, 0.9, 0.8729858381132479, 0.8]

    # ---- Manifold MixUp ----
    data.augment.manifold_mixup.prob:
      type: uniform
      lower: 0.0
      upper: 1.0
    data.augment.manifold_mixup.alpha:
      type: uniform
      lower: 0.3
      upper: 6.0
    data.augment.manifold_mixup.detach_pair:
      type: choice
      values: [false, true]

    # ---- Extra augmentation toggles (coarse) ----
    data.augment.watermark.prob:
      type: choice
      values: [0.0, 0.1, 0.2, 0.3]
    data.augment.light_spot.prob:
      type: choice
      values: [0.0, 0.1, 0.2, 0.3]
    data.augment.gaussian_noise.prob:
      type: choice
      values: [0.0, 0.25, 0.5]
    data.augment.gaussian_noise.std:
      type: choice
      values: [0.0, 0.005, 0.01, 0.02]
    data.augment.random_erasing.p:
      type: choice
      values: [0.0, 0.1, 0.25]
    data.augment.no_augment_prob:
      type: choice
      values: [0.0, 0.05, 0.1]

    # ---- Optimization strategy ablations ----
    mtl.enabled:
      type: choice
      values: [true, false]
    pcgrad.enabled:
      type: choice
      values: [true, false]

