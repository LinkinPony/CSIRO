defaults:
  # Base training config (same as other Tune runs)
  - experiment@_global_: baseline_no_irish
  # Switch to Mamba head for this sweep
  - override /model/head: mamba
  # Enable LoRA for HPO runs (the baseline experiment keeps it disabled by default).
  - override /train/peft: lora
  - _self_

hydra:
  job:
    chdir: false
  output_subdir: null
  run:
    dir: .

# HPO should run a single split by default (fast). After selecting best config,
# rerun `train.py --config <best_cfg.yaml>` (or `train_hydra.py`) with train_all/kfold enabled.
kfold:
  enabled: false
  folds: null
train_all:
  enabled: false

trainer:
  max_epochs: 45
  limit_train_batches: 1500
  log_every_n_steps: 10
  checkpoint:
    every_n_epochs: 1
  # Prefer EMA during HPO so per-epoch metrics are less noisy.
  swa:
    enabled: false
  ema:
    enabled: true
    decay: 0.999
    update_every_n_steps: 1
    start_step: 0
    eval_with_ema: true
    apply_at_end: true
    trainable_only: true

data:
  # Reduce loader contention when running multiple trials.
  # Explicitly override dataset defaults (csiro.yaml uses batch_size=1).
  batch_size: 2
  num_workers: 8
  prefetch_factor: 2
  # Train:val = 8:2
  val_split: 0.2

ray:
  # For a Ray cluster started via `ray start ...`, use `auto`.
  # For Ray Client, use: "ray://<head_ip>:10001"
  address: auto

# Fixed defaults (non-search) for stability.
model:
  backbone_layers:
    layer_fusion: learned

peft:
  use_dora: false
  target_modules: [qkv]
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  lora_llrd: 0.9

tune:
  name: tune-mamba-v1
  resume: false
  restore_path: null
  resume_unfinished: true
  resume_errored: true
  restart_errored: false
  storage_path: ray_results

  # Optimize r^2 directly (Ray metric name matches Lightning log name).
  metric: val_r2
  mode: max

  num_samples: 40
  max_concurrent_trials: 2
  resources_per_trial:
    cpu: 8
    gpu: 1

  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    # Keep a decent minimum training length before pruning (epoch reporting is 1-based).
    grace_period: 15
    reduction_factor: 2

  seeds: [42]
  report_per_epoch: true

  two_stage:
    enabled: true
    scope: best
    min_epoch: ${tune.scheduler.grace_period}
    top_n: 8
    name_suffix: "-stage2-kfold"
    resume: false
    restore_path: null
    resume_unfinished: true
    resume_errored: true
    restart_errored: false
    max_concurrent_trials: ${tune.max_concurrent_trials}
    resources_per_trial: ${tune.resources_per_trial}
    train_overrides:
      kfold.enabled: true
      kfold.k: 5
      kfold.even_split: false
      kfold.folds: null
      train_all.enabled: false

  # Search space spec. Keys are dotted paths into the training config.
  search_space:
    # ---- Mamba head (axial scan + pooled MLP) ----
    model.head.mamba.dim:
      type: choice
      values: [256, 320, 512, 640]
    model.head.mamba.depth:
      type: choice
      values: [2, 4, 6, 8]
    model.head.mamba.d_conv:
      type: choice
      values: [3, 5, 7]
    model.head.mamba.bidirectional:
      type: choice
      values: [true, false]

    model.head.hidden_dims:
      type: choice
      values:
        - []          # linear head on pooled features
        - [256]
        - [512]
        - [1024]
        - [512, 256]
        - [1024, 512]
        - [1280]
    model.head.activation:
      type: choice
      values: [relu, gelu, silu]
    model.head.dropout:
      type: uniform
      lower: 0.0
      upper: 0.5
    model.head.ratio_head_mode:
      type: choice
      values: [shared, separate_mlp, separate_spatial]

    # Multi-layer fusion when `model.backbone_layers.enabled=true` (baseline uses multi-layer).
    model.backbone_layers.layer_fusion:
      type: choice
      values: [mean, learned]

    # ---- LoRA (backbone adapters) ----
    peft.last_k_blocks:
      type: choice
      values: [2, 4, 6, 8, 12, 16]
    peft.r:
      type: choice
      values: [4, 8, 16]
    peft.lora_alpha:
      type: choice
      values: [8, 16, 32]
    peft.lora_dropout:
      type: uniform
      lower: 0.0
      upper: 0.15
    peft.lora_llrd:
      type: uniform
      lower: 0.8
      upper: 1.0
    peft.lora_lr:
      type: loguniform
      lower: 1.0e-5
      upper: 3.0e-3

    # ---- Optimizer / scheduler ----
    optimizer.lr:
      type: loguniform
      lower: 1.0e-5
      upper: 3.0e-3
    optimizer.weight_decay:
      type: loguniform
      lower: 1.0e-6
      upper: 3.0e-3
    scheduler.warmup_epochs:
      type: choice
      values: [1, 3, 5]

