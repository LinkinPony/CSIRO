defaults:
  # Base training config (same as other Tune runs)
  - experiment@_global_: baseline_no_irish
  # Switch to ViTDet-style head for this sweep
  - override /model/head: vitdet
  # Enable LoRA for HPO runs (the baseline experiment keeps it disabled by default).
  - override /train/peft: lora
  - _self_

hydra:
  job:
    chdir: false
  output_subdir: null
  run:
    dir: .

# HPO should run a single split by default (fast). After selecting best config,
# rerun `train.py --config <best_cfg.yaml>` (or `train_hydra.py`) with train_all/kfold enabled.
kfold:
  enabled: false
  folds: null
train_all:
  enabled: false

trainer:
  max_epochs: 45
  limit_train_batches: 1500
  log_every_n_steps: 10
  checkpoint:
    every_n_epochs: 1
  # NOTE: SWA is left enabled by default (from `conf/train/trainer/default.yaml`), and the repo
  # includes an adaptive SWA LR patch to avoid LR "jumps" at SWA start (see `AdaptiveSwaLrsOnStart`).

data:
  # Reduce loader contention when running multiple trials across the 2 machines.
  num_workers: 8
  prefetch_factor: 2

ray:
  # For a Ray cluster started via `ray start ...`, use `auto`.
  # For Ray Client, use: "ray://<head_ip>:10001"
  address: auto

# Fixed (non-search) defaults based on the r^2 analysis of the previous sweep:
# - Avoid GELU (high failure rate at larger LRs)
# - Prefer qkv-only LoRA modules (best mean r^2)
# - Disable DoRA (small negative average effect)
# - Fix ViTDet scale factors to the strongest mean choice
model:
  backbone_layers:
    layer_fusion: learned
  head:
    vitdet_scale_factors: [2.0, 1.0, 0.5]

peft:
  use_dora: false
  target_modules: [qkv]
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  lora_llrd: 0.9

tune:
  name: tune-vitdet-v2-r2-fast
  resume: false
  restore_path: null
  resume_unfinished: true
  resume_errored: true
  restart_errored: false
  storage_path: ray_results

  # Optimize r^2 directly (Ray metric name matches Lightning log name).
  metric: val_r2
  mode: max

  # Smaller search space => fewer samples needed; override from CLI if you want longer.
  num_samples: 40
  max_concurrent_trials: 2
  resources_per_trial:
    cpu: 8
    gpu: 1

  scheduler:
    type: asha
    max_epochs: ${trainer.max_epochs}
    # Keep a decent minimum training length before pruning (epoch reporting is 1-based).
    grace_period: 15
    reduction_factor: 2

  seeds: [42]
  report_per_epoch: true

  # Reduced search space (keep only the high-impact dimensions).
  # Keys are dotted paths into the training config.
  search_space:
    # ---- Optimizer / scheduler ----
    optimizer.lr:
      type: loguniform
      lower: 1.0e-5
      upper: 3.0e-3   # prune unstable high-LR region from previous sweep
    optimizer.weight_decay:
      type: loguniform
      lower: 1.0e-6
      upper: 3.0e-3
    scheduler.warmup_epochs:
      type: choice
      values: [1, 3, 5]

    # ---- ViTDet head ----
    model.head.vitdet_dim:
      type: choice
      values: [256, 320]
    model.head.hidden_dims:
      type: choice
      values:
        - []            # linear head on pooled pyramid features
        - [512, 256]
        - [1024, 512]
    model.head.activation:
      type: choice
      values: [relu, silu]
    model.head.dropout:
      type: uniform
      lower: 0.05
      upper: 0.35
    model.head.ratio_head_mode:
      type: choice
      values: [shared, separate_mlp, separate_spatial]

    # ---- LoRA ----
    peft.last_k_blocks:
      type: choice
      values: [4, 6, 12, 16]
    peft.lora_lr:
      type: loguniform
      lower: 3.0e-5
      upper: 3.0e-3


