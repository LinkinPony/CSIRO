seed: 42
version: tabpfn-2.5-tabular-v2
# This config is for the TabPFN (2.5) demo (`tabpfn_train.py`).
# - TabPFN input X comes from **image-model head penultimate features** (pre-final-linear),
#   extracted using the weights defined by `image_features.train_config` + a head checkpoint.
# - Uses the SAME CSIRO train.csv pivoting logic as the main project.
# - Uses the SAME k-fold split logic as the main project (optionally grouped by (Sampling_Date, State)).
# - Evaluation matches the project: weighted R^2 computed in log-space across 5 biomass components.
#
# NOTE: TabPFN 2.5 weights are hosted as a gated model on HuggingFace. If you hit auth errors:
# - Accept terms at https://huggingface.co/Prior-Labs/tabpfn_2_5
# - Then run `hf auth login` or set HF_TOKEN
# - Or provide a local checkpoint via `tabpfn.model_path` / `--model-path`

# Version tag (mirrors `configs/train.yaml` semantics):


data:
  # Keep consistent with the main project (relative to repo root).
  root: data
  train_csv: train.csv

  # Keep consistent with the main project pivot filtering:
  # pivoted rows are dropped if any of these targets are missing for an image_id.
  #
  # The TabPFN demo still evaluates on the full 5D biomass vector:
  # [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, GDM_g, Dry_Total_g]
  target_order: [Dry_Total_g]

kfold:
  # Mirrors `configs/train.yaml` (k-fold cross-validation).
  enabled: true

  # Number of folds
  k: 5

  # IMPORTANT:
  # - false: classic k-fold style (each sample should be validated once) -> enables OOF aggregation.
  # - true : for each fold, generate a fresh random ~50/50 split (NOT classic k-fold).
  even_split: false

  # Default behavior in the main project: keep all samples from the same
  # (Sampling_Date, State) group in the same fold.
  group_by_date_state: true

trainer:
  # Lightning Trainer settings (mirrors `configs/train.yaml`).
  #
  # Used by TabPFN fine-tuning when `finetune.enabled: true`.
  # NOTE: `trainer.max_epochs` takes precedence over `finetune.max_epochs` if both are set.
  max_epochs: 10
  accelerator: auto
  devices: 1
  # Start with 32-true for stability; if you hit OOM, try 16-mixed.
  precision: 16-mixed
  # Control per-epoch steps. Accepts int (absolute batches) or float (fraction).
  limit_train_batches: 1.0
  log_every_n_steps: 1
  accumulate_grad_batches: 1
  # Prefer setting clip values here (train.yaml style). Falls back to finetune.gradient_clip if omitted.
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  deterministic: false
  enable_progress_bar: true

train_all:
  # Mirrors `configs/train.yaml`:
  # - When enabled, additionally run a single "train_all" fit using **all** pivoted samples
  #   and write artifacts under:
  #     <logging.log_dir>/<version>/train_all/
  #     <logging.ckpt_dir>/<version>/train_all/
  #
  # This is useful for producing a final fine-tuned TabPFN checkpoint (if finetune.enabled=true)
  # and/or running TabPFN on the full training set after evaluating via k-fold.
  enabled: true

logging:
  # Where logs/artifacts are written (mirrors `configs/train.yaml`).
  log_dir: outputs
  # Where checkpoints are written (mirrors `configs/train.yaml`).
  ckpt_dir: outputs/checkpoints
  # Keep consistent with the rest of the repo.
  use_loguru: true
  tensorboard:
    # Write TabPFN run metrics (kfold + finetune + train_all) to TensorBoard under:
    #   <logging.log_dir>/<version>/tensorboard/
    enabled: true

# ==========================================================
# Packaged inference artifacts (produced by `package_artifacts.py`)
#
# TabPFN is **not** trained during inference. Instead, `package_artifacts.py`:
#  - extracts train image features
#  - fits TabPFN once
#  - saves fitted-state bundles under `artifacts.fit_state_dir`
#
# Inference (`infer_and_submit_pt.py`) then loads these bundles and only computes
# test features (optionally with ensemble + TTA).
# ==========================================================
artifacts:
  # Where to store fitted TabPFN state inside the packaged weights (relative to PROJECT_DIR).
  fit_state_dir: tabpfn_fit
  # Optional: store extracted train features used for fitting (relative to PROJECT_DIR).
  train_feature_cache_dir: tabpfn_train_features
  # Copy TabPFN foundation checkpoints into the packaged weights so fit-states can load offline.
  copy_tabpfn_weights: true

# ==========================================================
# Optional TabPFN fine-tuning (gradient-based).
#
# This uses TabPFN's built-in fine-tuning utilities:
# - get_preprocessed_datasets
# - fit_from_preprocessed + forward + backprop
#
# Fine-tuning runs inside each k-fold (if kfold.enabled=true) so behavior matches
# `configs/train.yaml` k-fold semantics.
# ==========================================================
finetune:
  enabled: false

  # Ensemble size used *during fine-tuning* (can be smaller than tabpfn.n_estimators for speed).
  # The fine-tuned weights can still be evaluated with a larger ensemble size afterwards.
  # NOTE: fine-tuning with n_estimators>1 is significantly more GPU-memory hungry because it builds
  # a larger autograd graph (multiple forward passes). Start with 1 to avoid OOM.
  n_estimators: 128

  # Precision for fine-tuning MUST be float32 for backprop (TabPFN disallows float64 in batched mode).
  inference_precision: float32

  # How many epochs to fine-tune per fold.
  max_epochs: 10

  # IMPORTANT: per-epoch evaluation can dominate wall time.
  # By default, the fine-tune loop evaluates on the **outer fold val** every epoch, and this evaluation
  # runs TabPFN inference 5x (one per target). If you use `tabpfn.n_estimators: 128`, evaluating with
  # 128 estimators each epoch can easily make epochs take ~minutes.
  #
  # Use a small ensemble here for fast feedback; final fold metrics are still computed with `tabpfn.n_estimators`.
  eval_n_estimators: 8

  # Evaluate on the outer fold validation set every N epochs (1 => every epoch).
  eval_every_n_epochs: 1

  # Meta-learning split inside each fold's training set:
  # - Each dataset item is split into (context/train) and (query/test) parts.
  inner_valid_ratio: 0.3

  # Meta-batch size (number of datasets per optimization step).
  # NOTE: TabPFN fine-tuning currently only supports meta_batch_size=1 reliably.
  meta_batch_size: 1

  # Limit samples per meta-dataset (helps align fine-tuning with inference context size).
  # - null: do not chunk; use all fold-train samples.
  # - int : chunk each (X,y) into pieces of at most this size before sampling meta-splits.
  # NOTE: lowering this can reduce peak GPU memory during fine-tuning.
  max_data_size: null
  equal_split_size: true

  # Optimizer
  optimizer:
    name: adam
    lr: 1.5e-6
    weight_decay: 0.0

  # Gradient clipping (like train.yaml)
  gradient_clip:
    enabled: true
    max_norm: 1.0

  # Checkpoint saving
  checkpoint:
    # Save fine-tuned TabPFN foundation weights under:
    #   <logging.ckpt_dir>/<version>/fold_<i>/tabpfn/
    enabled: true
    save_last: true
    save_best: true
    save_every_n_epochs: 0   # 0 => disabled
    monitor: val_weighted_r2_log

image_features:
  # Use the **image model config** (train.yaml) to define backbone + preprocessing.
  train_config: configs/train.yaml

  # TabPFN input feature source:
  # - head_penultimate: use per-fold head penultimate (pre-final-linear) features (requires fold head weights).
  # - dinov3_only: use frozen DINOv3 CLS token (no LoRA, no head; does not require any fold-specific weights).
  mode: head_penultimate
  # mode: dinov3_only

  # Head weights checkpoint used to compute penultimate features (z) inside the head.
  #
  # If null, `tabpfn_train.py` will best-effort auto-resolve:
  #  1) weights/head/infer_head.pt (if loadable)
  #  2) outputs/checkpoints/<image_config.version>/train_all/head/head-epoch*.pt (latest)
  #
  # No downloads are attempted.
  head_weights_path: null

  # How to reduce multi-layer head features to a single vector per image:
  # - mean  : use fused z (or mean of z_layers)  -> (D)
  # - concat: concatenate z_layers               -> (D * L)  (can exceed TabPFN feature limit)
  fusion: mean

  # Feature extraction dataloader params
  batch_size: 8
  num_workers: 8

  # Optional feature cache (speeds up repeated CV runs). When null, no caching.
  # cache_path: outputs/tabpfn_feature_cache/head_penultimate_features.pt
  cache_path: outputs/tabpfn_feature_cache/

tabpfn:

  # TabPFN ensemble size (n_estimators in TabPFNRegressor).
  n_estimators: 128

  # Device setting for TabPFN ("auto"|"cpu"|"cuda"|"cuda:0"|...)
  device: auto

  # TabPFN fit_mode: fit_preprocessors|low_memory|fit_with_cache|batched
  fit_mode: fit_preprocessors

  # Inference precision: auto|autocast|float16|bfloat16|float32
  inference_precision: auto

  # Post-process constraint for 5D predictions (ratio_strict):
  # 1) total = predicted Dry_Total_g
  # 2) total_sum = Dry_Clover_g + Dry_Dead_g + Dry_Green_g
  # 3) total_final = (total + total_sum) / 2
  # 4) Recompute (clover, dead, green) by preserving proportions and scaling to total_final
  # 5) gdm = clover + green
  ratio_strict: true

  # TabPFN has official limits on feature count (e.g. <=2000). Using DINO features can exceed this
  # (e.g. cls_patch_mean for vith16plus is 2*1280=2560), so we enable this override by default.
  ignore_pretraining_limits: true

  # MultiOutputRegressor parallelism (each output trains a separate TabPFNRegressor).
  # Keep 1 by default to avoid heavy duplication of model downloads/memory.
  n_jobs: 1

  # Telemetry: keep disabled by default.
  enable_telemetry: false

  # Optional: override cache directory for downloaded TabPFN models.
  # model_cache_dir: /path/to/cache
  model_cache_dir: null

  # REQUIRED: explicit local checkpoint path.
  # This script will NOT attempt to download weights.
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_default.ckpt
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_real-variant.ckpt
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_small-samples.ckpt
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_low-skew.ckpt
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_quantiles.ckpt
  model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_real.ckpt
  # Optional: explicit TabPFN python source path (offline-friendly). If empty, inference auto-falls back to
  # PROJECT_DIR/third_party/TabPFN/src when available.
  python_path: third_party/TabPFN/src
  # model_path: tabpfn_weights/tabpfn-v2.5-regressor-v2.5_variant.ckpt


#R^2 (base, n_estimators = 8)
#default:  0.511599
#real-variant: 0.508902
#small-samples: 0.500425
#low-skew: 0.516895
#quantiles: 0.505603
#real: 0.519799
#variant: 0.505333

#R^2 (base, n_estimators = 128)
#default:  0.514044
#real-variant: 0.511311
#small-samples: 0.507878
#low-skew: 
#quantiles: 
#real: 0.525890
#variant: 

#R^2 (mlp-patch-augmix, n_estimators = 128)
#default:  image=0.665844 tabpfn=0.684161 Δ=+0.018317
#real-variant: 
#small-samples: 
#low-skew: image=0.665844 tabpfn=0.687738 Δ=+0.021894
#quantiles: 
#real: .665844 tabpfn=0.684756 Δ=+0.018911
#variant: