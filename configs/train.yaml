seed: 42
version: h16p-lora-960-480-ndvi-kfold

data:
  root: data
  train_csv: train.csv
  image_size: [960, 480]
  batch_size: 1
  val_batch_size: 1
  num_workers: 20
  prefetch_factor: 4
  val_split: 0.1
  shuffle: true
  target_order: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g]
  augment:
    random_resized_crop_scale: [0.8, 1.0]
    horizontal_flip_prob: 0.5
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

model:
  backbone: dinov3_vith16plus
  # If weights_url is provided, it will be used to load the checkpoint.
  # Otherwise the official default weights are used via torch.hub.
  weights_url: null
  # Provide a local path to an offline checkpoint to avoid any network access.
  # Example: /path/to/dinov3_vith16plus_pretrain.pth
  weights_path: /media/dl/dataset/weights/dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pth
  pretrained: true
  freeze_backbone: true
  embedding_dim: 1280
  head:
    type: mlp
    hidden_dims: [640]
    activation: relu
    dropout: 0.2
    use_output_softplus: true

mtl:
  enabled: true
  tasks:
    height: false
    ndvi: true
    species: false
    state: false

train_all:
  enabled: false

peft:
  enabled: true
  method: lora
  use_dora: true
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  init: true
  last_k_blocks: 0
  layers_pattern: blocks
  target_modules: [qkv, proj, w1, w2, w3]
  lora_lr: 0.0005
  lora_weight_decay: 0.0

loss:
  weighting: uw

optimizer:
  name: adamw
  lr: 0.001
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_epochs: 1

kfold:
  enabled: true
  k: 5
  even_split: false

trainer:
  max_epochs: 30
  accelerator: auto
  devices: 1
  precision: 16-mixed
  log_every_n_steps: 1
  resume_from: null
  accumulate_grad_batches: 8
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  swa:
    enabled: true
    swa_lrs: 0.0005
    swa_epoch_start: 0.8
    annealing_epochs: 5
    annealing_strategy: cos

logging:
  log_dir: outputs
  ckpt_dir: outputs/checkpoints
  use_loguru: true
