seed: 42
version: manifold-mixup-disable
data:
  root: data
  train_csv: train.csv
  # Dataset selection and per-dataset physical area (meters)
  dataset: csiro
  datasets:
    csiro:
      width_m: 0.7
      length_m: 0.3
  image_size: [960, 480]
  batch_size: 1
  val_batch_size: 1
  num_workers: 20
  prefetch_factor: 4
  val_split: 0.1
  shuffle: true
  # Primary regression targets for the main head.
  # In the new design we predict only Dry_Total_g directly and recover
  # the remaining components via a separate ratio head.
  target_order: [Dry_Total_g]
  augment:
    random_resized_crop_scale: [0.8, 1.0]
    random_resized_crop:
      enabled: true  # Set to false to disable RandomResizedCrop (uses deterministic resize instead)
    horizontal_flip_prob: 0.5
    color_jitter:
      enabled: true
      prob: 0.8
      brightness: 0.2
      contrast: 0.2
      saturation: 0.1
      hue: 0.00
    vertical_flip:
      enabled: true
      prob: 0.5
    random_affine:
      enabled: true
      degrees: 5.0
      translate: [0.02, 0.02]
      scale: [0.95, 1.05]
      shear: [0.02, 0.02]
      interpolation: bilinear
      fill: 0
    gaussian_blur:
      enabled: true
      kernel_size: 3
      kernel_size_range: [3, 7]
      prob: 0.5
      sigma: [0.1, 1.0]
    gaussian_noise:
      enabled: true
      mean: 0.0
      std: 0.01
      prob: 0.5
    watermark:
      enabled: true
      prob: 0.3
      texts: []
      use_random_text: true
      random_text_length_range: [3, 20]
      timestamp_prob: 0.5
      font_size_frac_range: [0.04, 0.12]
      alpha_range: [128, 200]
      # color_choices: [[255,255,255],[255,230,0],[0,255,255],[255,128,0]]
    light_spot:
      enabled: true
      prob: 0.3
      radius_frac_range: [0.06, 0.18]
      alpha_range: [0.2, 0.6]
      color: [255, 255, 220]
      blur_frac: 0.5
    random_erasing:
      enabled: true
      p: 0.25
      scale: [0.02, 0.1]
      ratio: [0.3, 3.3]
      value: random
    # Feature-level manifold mixup on DINO backbone outputs
    # (CLS tokens, patch tokens, or global CLS+mean(patch) features)
    manifold_mixup:
      enabled: false
      prob: 0.5
      alpha: 0.3
    cutmix:
      enabled: false
      alpha: 0.5
      # minmax: [0.3, 0.7]  # optional clamp for bbox size; comment to disable
      use_prev_for_bsz1: true
    no_augment_prob: 0.1
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

model:
  backbone: dinov3_vith16plus
  # If weights_url is provided, it will be used to load the checkpoint.
  # Otherwise the official default weights are used via torch.hub.
  weights_url: null
  # Provide a local path to an offline checkpoint to avoid any network access.
  # Example: /path/to/dinov3_vith16plus_pretrain.pth
  weights_path: /media/dl/dataset/weights/dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pth
  pretrained: true
  freeze_backbone: true
  embedding_dim: 1280
  log_scale_targets: false
  head:
    type: mlp
    hidden_dims: [1280]
    activation: relu
    dropout: 0.2
    use_output_softplus: true
    # When enabled, use a patch-based main regression path:
    #   - For each patch token, apply the shared MLP head (CLS concatenated with patch token),
    #   - Average per-patch predictions to obtain the image-level output.
    # Auxiliary tasks (height/NDVI/species/state) and ratio/5D losses continue to use
    # the global CLS + mean(patch) bottleneck.
    use_patch_reg3: false
  backbone_layers:
    enabled: true
    # When true, each selected backbone layer uses its own bottleneck MLP.
    # When false, all layers share a single bottleneck (legacy behavior).
    separate_bottlenecks: true
    # Indices refer to DINOv3 transformer blocks (0-based). For dinov3_vith16plus
    # the depth is 32, so valid indices are in [0, 31]. Example: use the last 4 layers.
    indices: [25,27,29,31]

mtl:
  enabled: true
  tasks:
    height: false
    ndvi: false
    ndvi_dense: false
    species: false
    state: false
  # Per-step sampling ratio within an epoch (expected proportion).
  # ndvi_dense: probability to include NDVI-dense loss on a training step.
  sample_ratio:
    ndvi_dense: 0.0

train_all:
  enabled: true

peft:
  enabled: true
  method: lora
  use_dora: true
  r: 4
  lora_alpha: 4
  lora_dropout: 0.05
  init: true
  last_k_blocks: 0
  layers_pattern: blocks
  target_modules: [qkv, proj, w1, w2, w3]
  lora_lr: 0.0005
  lora_weight_decay: 0.0

loss:
  weighting: uw
  # Enable/disable the biomass ratio head (Dry_Clover_g, Dry_Dead_g, Dry_Green_g)
  use_ratio_head: true
  # Enable/disable 5D weighted MSE loss computed on:
  # [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, GDM_g, Dry_Total_g]
  use_5d_weighted_mse: true
  # Per-component weights for the 5D MSE term.
  mse_5d_weights_per_target: [0.1, 0.1, 0.1, 0.2, 0.5]
  # Optional whole-image consistency between global (CLS+mean(patch)) and patch-based
  # main heads for reg3. Only has an effect when model.head.use_patch_reg3: true.
  use_reg3_consistency: false
  reg3_consistency_weight: 1.0

optimizer:
  name: adamw            # choices: adamw, sgd, sam (alias for AdamW + SAM)
  lr: 0.001
  weight_decay: 0.01
  momentum: 0.9          # used only when name: sgd
  nesterov: false        # used only when name: sgd
  uw_lr: 0.01
  uw_weight_decay: 0.0
  # SAM optimizer (Sharpness-Aware Minimization) configuration.
  # To enable SAM, either set `name: sam` or `use_sam: true`.
  use_sam: false
  sam_rho: 0.1
  sam_adaptive: true

scheduler:
  name: cosine
  warmup_epochs: 0
  warmup_start_factor: 0.1

kfold:
  enabled: false
  k: 5
  even_split: true

trainer:
  max_epochs: 15
  accelerator: auto
  devices: 1
  precision: 16-mixed
  # Control per-epoch steps. Accepts int (absolute batches) or float (fraction).
  limit_train_batches: 900
  # With train_all enabled, we default to 1 so that validation uses only the dummy batch.
  limit_val_batches: 1
  log_every_n_steps: 1
  resume_from: null
  accumulate_grad_batches: 8
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  swa:
    enabled: true
    swa_lrs: 0.0001
    swa_epoch_start: 0.8
    annealing_epochs: 5
    annealing_strategy: cos

logging:
  log_dir: outputs
  ckpt_dir: outputs/checkpoints
  use_loguru: true

# Dense NDVI task configuration (tiles from large PNGs under data/NDVI/{carrot,onion}/)
ndvi_dense:
  enabled: false
  root: /media/dl/dataset/Git/CSIRO/data/NDVI
  tile_size: 640
  tile_stride: 448
  batch_size: 1
  num_workers: 8
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  augment:
    horizontal_flip_prob: 0.5
    # Optional per-task CutMix (falls back to data.augment.cutmix if omitted)
    cutmix:
      enabled: true
      prob: 0.5
      alpha: 0.3
      use_prev_for_bsz1: true
    vertical_flip_prob: 0.0

irish_glass_clover:
  enabled: false
  # Root directory containing data.csv and images/ for the Irish Glass Clover dataset
  root: data/irish_glass_clover
  csv: data.csv
  image_dir: images
  # Separate image size for this dataset (H, W). Augmentation config is reused from `data.augment`.
  # You can adjust this to match the native resolution of the Irish images.
  image_size: [800, 800]