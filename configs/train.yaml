seed: 42
version: ratio-mlp-2560-manifold-mixup
data:
  root: data
  train_csv: train.csv
  # Dataset selection and per-dataset physical area (meters)
  dataset: csiro
  datasets:
    csiro:
      width_m: 0.7
      length_m: 0.3
  image_size: [960, 480]
  batch_size: 1
  val_batch_size: 1
  num_workers: 20
  prefetch_factor: 4
  val_split: 0.1
  shuffle: true
  # Primary regression targets for the main head.
  # In the new design we predict only Dry_Total_g directly and recover
  # the remaining components via a separate ratio head.
  target_order: [Dry_Total_g]
  augment:
    random_resized_crop_scale: [0.8, 1.0]
    horizontal_flip_prob: 0.5
    vertical_flip:
      enabled: true
      prob: 0.5
    random_affine:
      enabled: true
      degrees: 5.0
      translate: [0.02, 0.02]
      scale: [0.95, 1.05]
      shear: [0.02, 0.02]
      interpolation: bilinear
      fill: 0
    gaussian_blur:
      enabled: true
      kernel_size: 3
      kernel_size_range: [3, 7]
      prob: 0.5
      sigma: [0.1, 1.0]
    gaussian_noise:
      enabled: true
      mean: 0.0
      std: 0.01
      prob: 0.5
    watermark:
      enabled: true
      prob: 0.3
      texts: []
      use_random_text: true
      random_text_length_range: [3, 20]
      timestamp_prob: 0.5
      font_size_frac_range: [0.04, 0.12]
      alpha_range: [128, 200]
      # color_choices: [[255,255,255],[255,230,0],[0,255,255],[255,128,0]]
    light_spot:
      enabled: true
      prob: 0.3
      radius_frac_range: [0.06, 0.18]
      alpha_range: [0.2, 0.6]
      color: [255, 255, 220]
      blur_frac: 0.5
    random_erasing:
      enabled: true
      p: 0.25
      scale: [0.02, 0.1]
      ratio: [0.3, 3.3]
      value: random
    # Feature-level manifold mixup on the shared bottleneck representation z
    manifold_mixup:
      enabled: true
      prob: 1.0
      alpha: 1.0
    cutmix:
      enabled: false
      alpha: 0.5
      # minmax: [0.3, 0.7]  # optional clamp for bbox size; comment to disable
      use_prev_for_bsz1: true
    no_augment_prob: 0.1
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

model:
  backbone: dinov3_vith16plus
  # If weights_url is provided, it will be used to load the checkpoint.
  # Otherwise the official default weights are used via torch.hub.
  weights_url: null
  # Provide a local path to an offline checkpoint to avoid any network access.
  # Example: /path/to/dinov3_vith16plus_pretrain.pth
  weights_path: /media/dl/dataset/weights/dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pth
  pretrained: true
  freeze_backbone: true
  embedding_dim: 1280
  log_scale_targets: false
  head:
    type: mlp
    hidden_dims: [2560]
    activation: relu
    dropout: 0.5
    use_output_softplus: true

mc_dropout:
  enabled: true
  # Number of stochastic forward passes of the regression head with dropout enabled
  # for Monte-Carlo dropout inference. When <= 1, falls back to deterministic head
  # evaluation as in the original script.
  num_samples: 256
  # When true, infer_and_submit_pt.py will write an additional
  # <submission>_mc_dropout_debug.json file with per-sample draws
  # and mean/variance/std statistics for MC dropout predictions.
  debug: false

mtl:
  enabled: true
  tasks:
    height: false
    ndvi: true
    ndvi_dense: false
    species: false
    state: false
  # Per-step sampling ratio within an epoch (expected proportion).
  # ndvi_dense: probability to include NDVI-dense loss on a training step.
  sample_ratio:
    ndvi_dense: 0.0

train_all:
  enabled: false

peft:
  enabled: true
  method: lora
  use_dora: true
  r: 4
  lora_alpha: 4
  lora_dropout: 0.05
  init: true
  last_k_blocks: 0
  layers_pattern: blocks
  target_modules: [qkv, proj, w1, w2, w3]
  lora_lr: 0.0005
  lora_weight_decay: 0.0

loss:
  #uw
  weighting: uw
  # Enable/disable the biomass ratio head (Dry_Clover_g, Dry_Dead_g, Dry_Green_g)
  use_ratio_head: true
  # KL divergence weight for the ratio head
  ratio_kl_weight: 1.0
  # Enable/disable 5D weighted MSE loss computed on:
  # [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, GDM_g, Dry_Total_g]
  use_5d_weighted_mse: true
  # Global multiplier for the 5D weighted MSE term
  mse_5d_weight: 1.0
  # Per-component weights for the 5D MSE term.
  mse_5d_weights_per_target: [0.1, 0.1, 0.1, 0.2, 0.5]

optimizer:
  name: adamw
  lr: 0.001
  weight_decay: 0.01
  uw_lr: 0.01
  uw_weight_decay: 0.0

scheduler:
  name: cosine
  warmup_epochs: 0
  warmup_start_factor: 0.1

kfold:
  enabled: true
  k: 5
  even_split: true

trainer:
  max_epochs: 15
  accelerator: auto
  devices: 1
  precision: 16-mixed
  # Control per-epoch steps. Accepts int (absolute batches) or float (fraction).
  limit_train_batches: 900
  # With train_all enabled, we default to 1 so that validation uses only the dummy batch.
  limit_val_batches: 1
  log_every_n_steps: 1
  resume_from: null
  accumulate_grad_batches: 8
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  swa:
    enabled: true
    swa_lrs: 0.0001
    swa_epoch_start: 0.8
    annealing_epochs: 5
    annealing_strategy: cos

logging:
  log_dir: outputs
  ckpt_dir: outputs/checkpoints
  use_loguru: true

# Dense NDVI task configuration (tiles from large PNGs under data/NDVI/{carrot,onion}/)
ndvi_dense:
  enabled: false
  root: /media/dl/dataset/Git/CSIRO/data/NDVI
  tile_size: 640
  tile_stride: 448
  batch_size: 1
  num_workers: 8
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  augment:
    horizontal_flip_prob: 0.5
    # Optional per-task CutMix (falls back to data.augment.cutmix if omitted)
    cutmix:
      enabled: true
      prob: 0.5
      alpha: 0.3
      use_prev_for_bsz1: true
    vertical_flip_prob: 0.0

irish_glass_clover:
  enabled: false
  # Root directory containing data.csv and images/ for the Irish Glass Clover dataset
  root: data/irish_glass_clover
  csv: data.csv
  image_dir: images
  # Separate image size for this dataset (H, W). Augmentation config is reused from `data.augment`.
  # You can adjust this to match the native resolution of the Irish images.
  image_size: [640, 640]